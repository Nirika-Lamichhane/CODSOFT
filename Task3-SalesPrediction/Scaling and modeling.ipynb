{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this involves standardization and normalization and is important as some models are sensitive to the feature scales\n",
    "\n",
    "'''\n",
    "for the small datasets the numbers with very high fluctuations can distort the model so better to do scaling\n",
    "standard is used when the datas are normally distibuted and bell shaped\n",
    "minmax is rare and for some specific requirements only\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression # this beacause our dataset is smaller and it helps prevent overfitting\n",
    "from sklearn.compose import ColumnTransformer # using this to broaden my knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908c5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# separate target and features as scaling is done in features\n",
    "x=df[['TV','Radio','Newspaper']] # for selecting more columns we have to use double squared\n",
    "y=df['Sales']\n",
    "\n",
    "# for feature scaling we have to split the datasets into train and test to prevent the data leakage and donot do cheating\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480e4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler using columntransformer\n",
    "\n",
    "# this is a object and blueprint of what actually has to be done (no caluculations)\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        # tuple used as we might apply different methods i.e. scaling or encoding according to the columns\n",
    "        ('scaled', StandardScaler(),['TV','Radio','Newspaper'])\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough' # this keeps the unused columns as it is without removing\n",
    ")\n",
    "\n",
    "# now scaling train and test data which will be numpy array\n",
    "\n",
    "# fit transform done so scaler lerans about the statisitcs of training data and tranforms / scales them\n",
    "x_train_Scaled= scaler.fit_transform(x_train) \n",
    "\n",
    "# only transforming test data as fiting will leak the datas because we are predicting the test data and doing fiting will help model to see data previously.\n",
    "x_test_Scaled= scaler.transform(x_test) \n",
    "\n",
    "# converting these arrays to the dataframe i.e. creates new dataframe\n",
    "x_train_Scaled = pd.DataFrame(x_train_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "x_test_Scaled=pd.DataFrame(x_test_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29112246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply minmax scaler to compare\n",
    "minmax=MinMaxScaler()\n",
    "\n",
    "x_train_mm=minmax.fit_transform(x_train)\n",
    "x_test_mm = minmax.transform(x_test)\n",
    "\n",
    "# converting the numpy arrays from fit transform to datframe\n",
    "x_train_df = pd.DataFrame(x_train_mm, columns=x.columns) \n",
    "# this x.columns sues the all columns original of the splited x feature i.e. the easiest way to do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d39651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard scaler summary\n",
      "              TV      Radio  Newspaper\n",
      "count  60.000000  60.000000  60.000000\n",
      "mean   -0.372680  -0.059658  -0.030828\n",
      "std     1.041877   1.058536   1.045048\n",
      "min    -1.806099  -1.558298  -2.798888\n",
      "25%    -1.241205  -1.080009  -0.619956\n",
      "50%    -0.551310  -0.042857   0.085558\n",
      "75%     0.511195   0.812578   0.703035\n",
      "max     1.632910   1.774300   1.874676\n",
      "\n",
      "\n",
      " summary statistics minmax\n",
      "               TV       Radio   Newspaper\n",
      "count  140.000000  140.000000  140.000000\n",
      "mean     0.526511    0.474294    0.617848\n",
      "std      0.283733    0.295072    0.221540\n",
      "min      0.000000    0.000000    0.000000\n",
      "25%      0.312225    0.216734    0.464561\n",
      "50%      0.559689    0.462702    0.651114\n",
      "75%      0.752621    0.742440    0.793081\n",
      "max      1.000000    1.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "# now comparing the two scaling models\n",
    "print ('standard scaler summary')\n",
    "print(x_test_Scaled.describe())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(' summary statistics minmax')\n",
    "print(x_train_df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aa69a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score [0.87526489 0.88616121 0.87006026 0.92191158 0.85014826]\n",
      "rmse score: [1.71442201 1.62232002 1.86918322 1.48730651 1.9977667 ]\n"
     ]
    }
   ],
   "source": [
    "# modelling starts from here\n",
    "# using linear regression\n",
    "import numpy as np\n",
    "model=LinearRegression()\n",
    "'''\n",
    "model.fit(x_train_Scaled, y_train) # this learns from the trained data\n",
    "\n",
    "y_pred=model.predict(x_test_Scaled)\n",
    "rmse=np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Lr without cross validation\")\n",
    "print(\"rmse: \", rmse)\n",
    "'''\n",
    "\n",
    "# linear regression with cross validation\n",
    "''' \n",
    "cross valdiation expects a score where higher is better and it actually expects the higher values rather than the smaller \n",
    "so when we use mse, it is error and we need it to be smaller not large as cv always search for larger\n",
    "so if we use neagtive mse then it will pick larger which iis the less mse thus best model\n",
    "'''\n",
    "corssV=cross_val_score(model, x_train_Scaled,y_train,cv=5,scoring='neg_mean_squared_error' )\n",
    "rmse= np.sqrt(-corssV)\n",
    "r2= cross_val_score(model, x_train_Scaled, y_train,scoring='r2',cv=5)\n",
    "print('r2 score',r2)\n",
    "print(\"rmse score:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7429f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best fold is of:  3\n",
      "rmse final model  1.5941556429410328\n",
      "r2 final model 0.9091588655128557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\OneDrive\\Desktop\\codsoft_projects\\CODSOFT\\Task3-SalesPrediction\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now analysing the best fold and refiting the model using that\n",
    "\n",
    "best= np.argmin(rmse)   # this np.argmin returns the index of the smallest value in np array\n",
    "print(\"best fold is of: \", best) # we got the index of best\n",
    "'''\n",
    "cv gives us the values of the best models and helps us identify the different sets\n",
    "but if we want to refit our model with this best dataset we have to manusally refit it\n",
    "cv uses kfold under the hood i.e. kfold is the default for splitting\n",
    "\n",
    "'''\n",
    "\n",
    "refit= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fd = list(refit.split(x_train_Scaled))  # this gives the list of the indexes of the splited folds ie.e list of all 5 folds\n",
    "\n",
    "# here happens the actual spliting of the dtaa to the best form\n",
    "trainIndex, valIndex =fd[best]  # this gives us the actual index of the best fold which is situated at the list given by kfold\n",
    "\n",
    "''' \n",
    "x_train_Scaled is the dataframe and indexing of dtaaframe is wrong so we have to convert it to the numpy array 1st and then only\n",
    "indexing is done\n",
    "'''\n",
    "x_train_Scaled_np =x_train_Scaled.values  # coverts to numpy\n",
    "y_train_np =y_train.values\n",
    "\n",
    "\n",
    "# this is doing indexing like a numpy array using integer array\n",
    "xFtrain, xFval=x_train_Scaled_np[trainIndex], x_train_Scaled_np[valIndex] # here splitting occurs\n",
    "\n",
    "yFtrain, yFval =y_train_np[trainIndex], y_train_np[valIndex]\n",
    "\n",
    "# noe fiting on this fold for the best model\n",
    "final=LinearRegression()\n",
    "final.fit(xFtrain, yFtrain)\n",
    "\n",
    "# evaluating metrics\n",
    "ypredf=final.predict(x_test_Scaled)\n",
    "rmsef= np.sqrt(mean_squared_error(y_test,ypredf))\n",
    "r2f= r2_score(y_test,ypredf)\n",
    "\n",
    "print(\"rmse final model \", rmsef)\n",
    "print('r2 final model',r2f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
