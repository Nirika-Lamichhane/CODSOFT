{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this involves standardization and normalization and is important as some models are sensitive to the feature scales\n",
    "\n",
    "'''\n",
    "for the small datasets the numbers with very high fluctuations can distort the model so better to do scaling\n",
    "standard is used when the datas are normally distibuted and bell shaped\n",
    "minmax is rare and for some specific requirements only\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso # this beacause our dataset is smaller and it helps prevent overfitting\n",
    "from sklearn.compose import ColumnTransformer # using this to broaden my knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908c5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# separate target and features as scaling is done in features\n",
    "x=df[['TV','Radio','Newspaper']] # for selecting more columns we have to use double squared\n",
    "y=df['Sales']\n",
    "\n",
    "# for feature scaling we have to split the datasets into train and test to prevent the data leakage and donot do cheating\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480e4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler using columntransformer\n",
    "\n",
    "# this is a object and blueprint of what actually has to be done (no caluculations)\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        # tuple used as we might apply different methods i.e. scaling or encoding according to the columns\n",
    "        ('scaled', StandardScaler(),['TV','Radio','Newspaper'])\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough' # this keeps the unused columns as it is without removing\n",
    ")\n",
    "\n",
    "# now scaling train and test data which will be numpy array\n",
    "\n",
    "# fit transform done so scaler lerans about the statisitcs of training data and tranforms / scales them\n",
    "x_train_Scaled= scaler.fit_transform(x_train) \n",
    "\n",
    "# only transforming test data as fiting will leak the datas because we are predicting the test data and doing fiting will help model to see data previously.\n",
    "x_test_Scaled= scaler.transform(x_test) \n",
    "\n",
    "# converting these arrays to the dataframe i.e. creates new dataframe\n",
    "x_train_Scaled = pd.DataFrame(x_train_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "x_test_Scaled=pd.DataFrame(x_test_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29112246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply minmax scaler to compare\n",
    "minmax=MinMaxScaler()\n",
    "\n",
    "x_train_mm=minmax.fit_transform(x_train)\n",
    "x_test_mm = minmax.transform(x_test)\n",
    "\n",
    "# converting the numpy arrays from fit transform to datframe\n",
    "x_train_df = pd.DataFrame(x_train_mm, columns=x.columns) \n",
    "# this x.columns sues the all columns original of the splited x feature i.e. the easiest way to do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d39651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard scaler summary\n",
      "              TV      Radio  Newspaper\n",
      "count  60.000000  60.000000  60.000000\n",
      "mean   -0.372680  -0.059658  -0.030828\n",
      "std     1.041877   1.058536   1.045048\n",
      "min    -1.806099  -1.558298  -2.798888\n",
      "25%    -1.241205  -1.080009  -0.619956\n",
      "50%    -0.551310  -0.042857   0.085558\n",
      "75%     0.511195   0.812578   0.703035\n",
      "max     1.632910   1.774300   1.874676\n",
      "\n",
      "\n",
      " summary statistics minmax\n",
      "               TV       Radio   Newspaper\n",
      "count  140.000000  140.000000  140.000000\n",
      "mean     0.526511    0.474294    0.617848\n",
      "std      0.283733    0.295072    0.221540\n",
      "min      0.000000    0.000000    0.000000\n",
      "25%      0.312225    0.216734    0.464561\n",
      "50%      0.559689    0.462702    0.651114\n",
      "75%      0.752621    0.742440    0.793081\n",
      "max      1.000000    1.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "# now comparing the two scaling models\n",
    "print ('standard scaler summary')\n",
    "print(x_test_Scaled.describe())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(' summary statistics minmax')\n",
    "print(x_train_df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa69a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
