{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6be332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this involves standardization and normalization and is important as some models are sensitive to the feature scales\n",
    "\n",
    "'''\n",
    "for the small datasets the numbers with very high fluctuations can distort the model so better to do scaling\n",
    "standard is used when the datas are normally distibuted and bell shaped\n",
    "minmax is rare and for some specific requirements only\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LinearRegression # this beacause our dataset is smaller and it helps prevent overfitting\n",
    "from sklearn.compose import ColumnTransformer # using this to broaden my knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# separate target and features as scaling is done in features\n",
    "x=df[['TV','Radio','Newspaper']] # for selecting more columns we have to use double squared\n",
    "y=df['Sales']\n",
    "\n",
    "# for feature scaling we have to split the datasets into train and test to prevent the data leakage and donot do cheating\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler using columntransformer\n",
    "\n",
    "# this is a object and blueprint of what actually has to be done (no caluculations)\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        # tuple used as we might apply different methods i.e. scaling or encoding according to the columns\n",
    "        ('scaled', StandardScaler(),['TV','Radio','Newspaper'])\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough' # this keeps the unused columns as it is without removing\n",
    ")\n",
    "\n",
    "# now scaling train and test data which will be numpy array\n",
    "\n",
    "# fit transform done so scaler lerans about the statisitcs of training data and tranforms / scales them\n",
    "x_train_Scaled= scaler.fit_transform(x_train) \n",
    "\n",
    "# only transforming test data as fiting will leak the datas because we are predicting the test data and doing fiting will help model to see data previously.\n",
    "x_test_Scaled= scaler.transform(x_test) \n",
    "\n",
    "# converting these arrays to the dataframe i.e. creates new dataframe\n",
    "x_train_Scaled = pd.DataFrame(x_train_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "x_test_Scaled=pd.DataFrame(x_test_Scaled,columns=['TV','Radio','Newspaper'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29112246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply minmax scaler to compare\n",
    "minmax=MinMaxScaler()\n",
    "\n",
    "x_train_mm=minmax.fit_transform(x_train)\n",
    "x_test_mm = minmax.transform(x_test)\n",
    "\n",
    "# converting the numpy arrays from fit transform to datframe\n",
    "x_train_df = pd.DataFrame(x_train_mm, columns=x.columns) \n",
    "# this x.columns sues the all columns original of the splited x feature i.e. the easiest way to do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now comparing the two scaling models\n",
    "print ('standard scaler summary')\n",
    "print(x_test_Scaled.describe())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(' summary statistics minmax')\n",
    "print(x_train_df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa69a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling starts from here\n",
    "# using linear regression\n",
    "import numpy as np\n",
    "model=LinearRegression()\n",
    "'''\n",
    "model.fit(x_train_Scaled, y_train) # this learns from the trained data\n",
    "\n",
    "y_pred=model.predict(x_test_Scaled)\n",
    "rmse=np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Lr without cross validation\")\n",
    "print(\"rmse: \", rmse)\n",
    "'''\n",
    "\n",
    "# linear regression with cross validation\n",
    "''' \n",
    "cross valdiation expects a score where higher is better and it actually expects the higher values rather than the smaller \n",
    "so when we use mse, it is error and we need it to be smaller not large as cv always search for larger\n",
    "so if we use neagtive mse then it will pick larger which iis the less mse thus best model\n",
    "'''\n",
    "corssV=cross_val_score(model, x_train_Scaled,y_train,cv=5,scoring='neg_mean_squared_error' )\n",
    "rmse= np.sqrt(-corssV)\n",
    "r2= cross_val_score(model, x_train_Scaled, y_train,scoring='r2',cv=5)\n",
    "print('r2 score',r2)\n",
    "print(\"rmse score:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7429f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now analysing the best fold and refiting the model using that\n",
    "\n",
    "best= np.argmin(rmse)   # this np.argmin returns the index of the smallest value in np array\n",
    "print(\"best fold is of: \", best) # we got the index of best\n",
    "'''\n",
    "cv gives us the values of the best models and helps us identify the different sets\n",
    "but if we want to refit our model with this best dataset we have to manusally refit it\n",
    "cv uses kfold under the hood i.e. kfold is the default for splitting\n",
    "\n",
    "'''\n",
    "\n",
    "refit= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fd = list(refit.split(x_train_Scaled))  # this gives the list of the indexes of the splited folds ie.e list of all 5 folds\n",
    "\n",
    "# here happens the actual spliting of the dtaa to the best form\n",
    "trainIndex, valIndex =fd[best]  # this gives us the actual index of the best fold which is situated at the list given by kfold\n",
    "\n",
    "''' \n",
    "x_train_Scaled is the dataframe and indexing of dtaaframe is wrong so we have to convert it to the numpy array 1st and then only\n",
    "indexing is done\n",
    "'''\n",
    "x_train_Scaled_np =x_train_Scaled.values  # coverts to numpy\n",
    "y_train_np =y_train.values\n",
    "\n",
    "\n",
    "# this is doing indexing like a numpy array using integer array\n",
    "xFtrain, xFval=x_train_Scaled_np[trainIndex], x_train_Scaled_np[valIndex] # here splitting occurs\n",
    "\n",
    "yFtrain, yFval =y_train_np[trainIndex], y_train_np[valIndex]\n",
    "\n",
    "# noe fiting on this fold for the best model\n",
    "final=LinearRegression()\n",
    "final.fit(xFtrain, yFtrain)\n",
    "\n",
    "# evaluating metrics\n",
    "ypredf=final.predict(x_test_Scaled)\n",
    "rmsef= np.sqrt(mean_squared_error(y_test,ypredf))\n",
    "r2f= r2_score(y_test,ypredf)\n",
    "\n",
    "print(\"rmse final model \", rmsef)\n",
    "print('r2 final model',r2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of linear regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, ypredf, alpha=0.7) # aplha means transparency \n",
    "\n",
    "# this line below gives the points from where the line should be plotted\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')   # this is as (x,y) so x is y_test.min and max and y is also y_test.min and max\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title(' Actual vs Predicted')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd742d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reisual plot to identify the outliers and errors\n",
    "\n",
    "residuals = y_test - ypredf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(ypredf, residuals, alpha=0.7)\n",
    "plt.hlines(0, ypredf.min(), ypredf.max(), colors='r', linestyles='dashed')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot for Lienar Regression')\n",
    "plt.show()\n",
    "\n",
    "''' \n",
    " this plot showed some outliers and errors in the model which might afftect the vairance assumptions \n",
    " so we have to standardize those residues mannually\n",
    "'''\n",
    "\n",
    "# standardizing the residuals\n",
    "stdresiduals=residuals / np.std(residuals) \n",
    "# this divides each residuals by their sd and if that is >3 then it is outlier as per by rule i.e. >+-3\n",
    "\n",
    "\n",
    "# now flag potential outliers \n",
    "outliers =np.where(np.abs(stdresiduals) > 3)[0]  \n",
    "'''\n",
    "this gives the index of the outliers\n",
    "the above line gives the tuple as (a,b) where a is the index of outlier and b denotes that 3 i.e. the number for which we are checking\n",
    "then [0] helps us  to get the index of outliers i.e. the first element of the tuple\n",
    "'''\n",
    "print(\"linear regression outliers at index: \", outliers)\n",
    "\n",
    "''' \n",
    "running this i found the outlier at index 34 i.e. from 0 to 34\n",
    "noe checking the value\n",
    "this index of outlier is the index of the test data and not the original dataset\n",
    "'''\n",
    "index= 34\n",
    "# Get the *true original index* from the dataset\n",
    "print(\"True index from original dataset:\", y_test.index[34])\n",
    "\n",
    "# Get the actual values:\n",
    "print(\"Actual value:\", y_test.iloc[34])\n",
    "print(\"Predicted value:\", ypredf[34])\n",
    "print(\"Residual:\", residuals.iloc[34])\n",
    "print(\"Standardized residual:\", stdresiduals.iloc[34])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "as this datseet is small and this one residual outier is high and for the preediction of the sales, it might cause error \n",
    "so we have to use more robust model like huber i.e. less sensitive to outliers\n",
    "it squares the less residues\n",
    "it gives the absolute value of the large residues\n",
    "it identifies and separates the outliers and normal values using epsilon value\n",
    "if e= 1.35 and is the absolute error is less than e then it is normal and huber squares them\n",
    "otherwise it treates as the outlier and use the absolute value\n",
    "\n",
    "'''\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "huber= HuberRegressor()\n",
    "huber.fit(x_train, y_train)\n",
    "ypredhuber=huber.predict(x_test)\n",
    "rmsehuber=np.sqrt(mean_squared_error(y_test, ypredhuber))\n",
    "r2huber=r2_score(y_test, ypredhuber)\n",
    "print(\"rmse huber: \", rmsehuber)\n",
    "print(\"r2 huber: \", r2huber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "x=df[['TV','Radio']]\n",
    "y=df['Sales']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y, test_size=0.3, random_state=42)\n",
    "\n",
    "# scaling these features\n",
    "scaler=StandardScaler()\n",
    "x_scaled_train=scaler.fit_transform(x_train)\n",
    "x_test_scaled= scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ridge regresion\n",
    "'''  \n",
    "alpha is the parameter that gives how much value to shrink. \n",
    "high value = underfiting more regularization, coefficients becomes smaller\n",
    "low = overfiting\n",
    "we have to tune this alpha value as it might result to the wrong value and affect our estimation\n",
    "to do this we can use the ridgecv i.e. the alpha tuner for ridge regression\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "alpha=np.logspace(-4,4,50) # this cretaes a  numpy array of 50 alphas from 10 pwer -4 to 4\n",
    "ridgecv = RidgeCV(alphas=alpha, cv=5,scoring='neg_mean_squared_error')\n",
    "\n",
    "# this line below selects the best alpha with the best average cv score and then fits the whole model on that alpha\n",
    "ridgecv.fit(x_scaled_train, y_train)  # this fits the model of data\n",
    "\n",
    "ypred=ridgecv.predict(x_test_scaled)  \n",
    "\n",
    "coefDF= pd.DataFrame({'Feature': x.columns, 'coefficient':ridgecv.coef_})\n",
    "# evaluation of model\n",
    "mse=mean_squared_error(y_test,ypred)\n",
    "\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "r2=r2_score(y_test,ypred)\n",
    "\n",
    "print(\"best alpha:\", ridgecv.alpha_)\n",
    "print(\"mse:\", mse)\n",
    "print(\"rmse:\", rmse)\n",
    "print(\"r2 score:\", r2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(y_test, ypred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line for perfect prediction\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Ridge Regression: Actual vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63250f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - ypred\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(ypred, residuals, alpha=0.7)\n",
    "plt.hlines(0, ypred.min(), ypred.max(), colors='r', linestyles='dashed')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot for Ridge Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44748d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of x.columns:\", len(x.columns))\n",
    "print(\"x.columns:\", list(x.columns))\n",
    "\n",
    "print(\"Shape of final.coef_:\", final.coef_.shape)\n",
    "print(\"final.coef_:\", final.coef_)\n",
    "\n",
    "print(\"Shape of ridgecv.coef_:\", ridgecv.coef_.shape)\n",
    "print(\"ridgecv.coef_:\", ridgecv.coef_)\n",
    "\n",
    "print(\"Length of Model list Linear Regression:\", len(['Linear Regression'] * len(x.columns)))\n",
    "print(\"Length of Model list Ridge Regression:\", len(['Ridge Regression'] * len(x.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b430398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of the coefficients\n",
    "\n",
    "# coefficients of the linear regression\n",
    "''' \n",
    "when we make the df from pandas from a dictionary, every list or column must be of the same length\n",
    "as features and coefficients are of length 3 but the model is of only 1 length string\n",
    "so either we have to drop this or we have to make this lenth 3\n",
    "so we did make this length 3\n",
    "here the x.columns has 2 length only as the x has 2 columns as tv and radio \n",
    "so we have to make this plot list with 2 models only\n",
    "\n",
    "'''\n",
    "print(final.coef_.shape)\n",
    "coeflr = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'coefficient': final.coef_[:len(x.columns)],        # shape (3,) matches x.columns length\n",
    "    'Model': ['Linear Regression'] * len(x.columns)\n",
    "})\n",
    "\n",
    "# coefficients of the ridge  regression\n",
    "coefridge = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'coefficient': ridgecv.coef_,\n",
    "    'Model': ['Ridge Regression'] * len(x.columns)\n",
    "})\n",
    "# combining both for plot\n",
    "coefall= pd.concat([coeflr, coefridge], ignore_index=True)\n",
    "\n",
    "# plotting the coefficients\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# hue splits the data with the model type and for each model coefficients will be sideways and color will be asssigned different\n",
    "sns.barplot(data=coefall, x='Feature', y='coefficient', hue='Model' , palette='viridis')\n",
    "\n",
    "plt.title('comparison between coefficients of Linear and Ridge Regression')\n",
    "plt.ylabel('coefficient')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    " this plot shows the coefficient of the varous features as caluclated by the linear i.e. non penalized and penalized i.e. ridge regression\n",
    "The ridge regression coefficients are smaller than the linear regression coefficients, indicating that ridge regression has applied regularization to reduce the magnitude of the coefficients, which helps prevent overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498c7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
