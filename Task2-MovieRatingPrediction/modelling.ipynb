{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('cleaned_encoded.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006dd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the  target and features for modelling\n",
    "\n",
    "#dropping the unwanted\n",
    "df=df.drop(['Name','Year','Duration','Votes','Genre','Actor 1','Actor 2','Actor 3','Director','Genre_list'],axis=1)\n",
    "\n",
    "# separate x and y\n",
    "x=df.drop('Rating',axis=1) # features\n",
    "y=df['Rating'] # target\n",
    "\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now trainng and testing split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.3, random_state=42)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "print(f\"Test shape: {x_test.shape}\")\n",
    "print(x_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbe547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this wrt rating\n",
    "'''\n",
    "this is done to check if the datas are skewed or not\n",
    "through histogram we saw the plot and both are bell shaped so no skew i.e. symmetric\n",
    "if tail longer on right = positive skewd\n",
    "if tail longer on left = negative skewed\n",
    "\n",
    "'''\n",
    "plt.figure(figsize=(10,4)) # creats canvas\n",
    "plt.subplot(1,2,1) # grid of 1 row 2 column and working on 1st subplot \n",
    "plt.hist(y_train, bins=20, color='skyblue', edgecolor='black')\n",
    "# this plt.hist() only plots the bars and doesnot include kde by default. for including we have to use seaborn\n",
    "# plt.title('Train Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('count') # this is number of data at each bins \n",
    "sns.histplot(df['Rating'], kde =True)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2) # now moving to subplot 2\n",
    "plt.hist(y_test, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Test Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('count')\n",
    "sns.histplot(df['Rating'], kde =True)\n",
    "\n",
    "# to show the skewness numerically\n",
    "print(df['Rating'].skew())  # this resulted in value less than 0.5 so it is not mild \n",
    "\n",
    "plt.tight_layout() # this brings the space between the 2 subplots so that they dont overlapp\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model using simple linear regression\n",
    "'''\n",
    "linear regression is the simple model that fits a straight line to predict our movie rating\n",
    ".fit() trains the model using the given datas it uses the mathematical optimization techniques and\n",
    "caluclate the best coefficients that minimize the prediction error and store this learned/ trained parameters for later use\n",
    "\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "\n",
    "model=LinearRegression() # this creates the empty model object ready to learn from the given datasets\n",
    "\n",
    "model.fit(x_train, y_train)  \n",
    "\n",
    "# coeff of each feature\n",
    "print(model.coef_) \n",
    "\n",
    "# intercept or bias term\n",
    "print( model.intercept_)\n",
    "\n",
    "# prediction value\n",
    "y_pred=model.predict(x_test)  \n",
    "'''\n",
    "this uses the trained model to predict on the new unseen and untrained data sets.\n",
    "here predict applies the learned equation  with the learned coefficient and intercept\n",
    "y_pred stores the arrays  y=b0+b1 x1+b2x2+b3x3 and so on is the simple equation on which linear regression works\n",
    "\n",
    "'''\n",
    "\n",
    "# now checking the errors between the predicton and the actual one\n",
    "\n",
    "print(\" mean absolute error :\", mean_absolute_error(y_test,y_pred))   # lower mse good model\n",
    "\n",
    "print(\"mean squared error: \",mean_squared_error(y_test,y_pred))\n",
    "# mse is squared form of the target variable so rmse is used to make that more interpreatable and compare directly\n",
    "\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred,squared=False )) # this squared false means to not return mse which is false but to return the square root of mse\n",
    "\n",
    "# now checking r2 square\n",
    "print(\"r2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
