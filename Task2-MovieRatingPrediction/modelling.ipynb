{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "\n",
    "# importing from ensemble module to check how the other algorithm react to the dataset\n",
    "# regrsor used instead of classifier as we have to predict a number i.e. continuous value where classifier predicts a category \n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # these all follows scikit learn rules also includes bagging classifier\n",
    "\n",
    "# this comes with the xgboost library and is built for high performance\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "# to improve efficiency using hypertuning paramter\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('cleaned_encoded.csv')\n",
    "print(list(df.columns))\n",
    "if 'actors' in df.columns:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006dd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the  target and features for modelling\n",
    "\n",
    "#dropping the unwanted\n",
    "df=df.drop(['Name','Year','Duration','Votes','Genre','Actor 1','Actor 2','Actor 3','Director','Genre_list','actors'],axis=1)\n",
    "\n",
    "# separate x and y\n",
    "x=df.drop('Rating',axis=1) # features\n",
    "y=df['Rating'] # target\n",
    "\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now trainng and testing split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.3, random_state=42)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "print(f\"Test shape: {x_test.shape}\")\n",
    "print(x_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbe547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this wrt rating\n",
    "'''\n",
    "this is done to check if the datas are skewed or not\n",
    "through histogram we saw the plot and both are bell shaped so no skew i.e. symmetric\n",
    "if tail longer on right = positive skewd\n",
    "if tail longer on left = negative skewed\n",
    "\n",
    "'''\n",
    "plt.figure(figsize=(10,4)) # creats canvas\n",
    "plt.subplot(1,2,1) # grid of 1 row 2 column and working on 1st subplot \n",
    "plt.hist(y_train, bins=20, color='skyblue', edgecolor='black')\n",
    "# this plt.hist() only plots the bars and doesnot include kde by default. for including we have to use seaborn\n",
    "# plt.title('Train Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('count') # this is number of data at each bins \n",
    "sns.histplot(df['Rating'], kde =True)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2) # now moving to subplot 2\n",
    "plt.hist(y_test, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Test Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('count')\n",
    "sns.histplot(df['Rating'], kde =True)\n",
    "\n",
    "# to show the skewness numerically\n",
    "print(df['Rating'].skew())  # this resulted in value less than 0.5 so it is not mild \n",
    "\n",
    "plt.tight_layout() # this brings the space between the 2 subplots so that they dont overlapp\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model using simple linear regression\n",
    "'''\n",
    "linear regression is the simple model that fits a straight line to predict our movie rating\n",
    ".fit() trains the model using the given datas it uses the mathematical optimization techniques and\n",
    "caluclate the best coefficients that minimize the prediction error and store this learned/ trained parameters for later use\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "model=LinearRegression() # this creates the empty model object ready to learn from the given datasets\n",
    "\n",
    "model.fit(x_train, y_train)  \n",
    "\n",
    "# coeff of each feature\n",
    "print(model.coef_) \n",
    "\n",
    "# intercept or bias term\n",
    "print( model.intercept_)\n",
    "\n",
    "# prediction value\n",
    "y_pred=model.predict(x_test)  \n",
    "'''\n",
    "this uses the trained model to predict on the new unseen and untrained data sets.\n",
    "here predict applies the learned equation  with the learned coefficient and intercept\n",
    "y_pred stores the arrays  y=b0+b1 x1+b2x2+b3x3 and so on is the simple equation on which linear regression works\n",
    "\n",
    "'''\n",
    "\n",
    "# now checking the errors between the predicton and the actual one\n",
    "\n",
    "print(\" mean absolute error :\", mean_absolute_error(y_test,y_pred))   # lower mse good model\n",
    "\n",
    "print(\"mean squared error: \",mean_squared_error(y_test,y_pred))\n",
    "# mse is squared form of the target variable so rmse is used to make that more interpreatable and compare directly\n",
    "'''\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred,squared=False )) # this squared false means to not return mse which is false but to return the square root of mse\n",
    "this is not supported by this version of scikit learn\n",
    "so we have to do manuually\n",
    "'''\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "print(\"rmse: \",rmse)\n",
    "# now checking r2 square\n",
    "print(\"r2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing residuals to detect outliers\n",
    "residuals= y_test- y_pred \n",
    "# this creates a scatter plot  \n",
    "plt.scatter(y_pred, residuals) # (x axis, y axis)\n",
    "\n",
    "# this line below draws the horizontal line at y=0 i.e. the position where prediction is perfect i.e. 0 error\n",
    "plt. axhline(y=0, color='b', linestyle='--') \n",
    "\n",
    "plt.xlabel(\"predicted value from model\")\n",
    "plt.ylabel('residuals')\n",
    "plt.title(\" residue plot to detect outlier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac091c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing actual and predicted data to know where is model struggling\n",
    "plt.figure(figsize=(10, 6)) # creates canvas\n",
    "\n",
    "# scatter plot using seaborn 0.6 means opacity so that overlapping points are still visible\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)\n",
    "\n",
    "# this plots the perfect prediction line as diagonal y=x because here the x axis and y axis moves from min to max value forming the diagonal like structure\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', label='Ideal fit of the model')\n",
    "\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()  # this is added for reference as it describes the labeled elements in the plot using their label\n",
    "plt.grid(True)  # this forms the gird so it will be easier to align the points to axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 3 different tree models\n",
    "''' \n",
    "instead of doing each one by one and seeing the values i have made this so as to calcualte each at once and compare sideways\n",
    "\n",
    "'''\n",
    "\n",
    "# defining the models in dictionary\n",
    "models={\n",
    "    \"randomForest\": RandomForestRegressor(n_estimators=100 ,random_state=42), # n_estimator = number of trees as random forest works on different tree so this helps in selecting the number\n",
    "    \"gradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "\n",
    "    # here we use verbosity as the xgboost is made up with c++ and python for the fast performance and this might be giving log message so using verbosity gives us contool of how much the model talks with us\n",
    "    \"xgboost\":XGBRegressor(n_estimators=100, random_state=42,verbosity=0)\n",
    "\n",
    "}\n",
    "\n",
    "# using grid search and hyperparamters  keys are the correct hyperparameter name\n",
    "hp={\n",
    "    \"randomForest\":{\n",
    "        \"n_estimators\":[100,200],\n",
    "        \"max_depth\":[10,20], # maximum depth of the tree\n",
    "    },\n",
    "    \"gradientBoosting\":{\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1], # as this learns fromm the past mistakes continuously\n",
    "\n",
    "    # learning rate is low as i dont want to jump directly to the top and miss out the middle values so to improve more accuracy steady it is low\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "    }\n",
    "    }\n",
    "\n",
    "results=[] # empty list to store result of each model\n",
    "\n",
    "\n",
    "# now train predict in this in a loop for each model\n",
    "for i,model in models.items():\n",
    "    '''\n",
    "    this line gives the key value pair i.e. .items() gives \n",
    "    i=key\n",
    "    and model= value\n",
    "    \n",
    " this below line is for the fixed set of estimators and training and testing size \n",
    "    model.fit(x_train,y_train)\n",
    "    y_pred=model.predict(x_test)\n",
    "\n",
    "    but for lettinf the user have different estimators and sets we use hyperparamters i.e. different numbers of various parameters\n",
    "    and to work on those values, model should be trained on those datas which is supported by grid search cv\n",
    "'''\n",
    "\n",
    "# hp[i] gives the list of hyperparameters for the values of key\n",
    "# cv is cross valdiation in which the whole data sets is divided into 8 parts and each part is divided as 7 train and 1 test and performed 8 times for stablility\n",
    "\n",
    "\n",
    "    grid=GridSearchCV(model,hp[i], cv=3 , scoring='r2', n_jobs=-1) # can use n_jobs (much needed when dealing with many models in loop) which helps in parallelization of cpu i.e. -1 means uses all 4 cores , 1 means only 1 core and likewise\n",
    "    grid.fit(x_train,y_train)\n",
    "     # these 2 lines above now has the output as:  grid.best_params_ , grid.best_score_ , grid_best_estimator_\n",
    "\n",
    "    modelbest= grid.best_estimator_\n",
    "    y_pred=modelbest.predict(x_test)\n",
    "\n",
    "    MAE= mean_absolute_error(y_test,y_pred)\n",
    "    MSE=mean_squared_error(y_test,y_pred)\n",
    "    RMSE=np.sqrt(MSE)\n",
    "    r2_Score= r2_score(y_test,y_pred)\n",
    "\n",
    "    # i am trying to make the table so as to ahow the values directly \n",
    "\n",
    "    results.append({\n",
    "        \"model\": i,\n",
    "        \"mae\":MAE,\n",
    "        \"mse\":MSE,\n",
    "        \"rmse\": RMSE,\n",
    "        \"r2 score\": r2_Score\n",
    "\n",
    "    })\n",
    "resultsdf=pd.DataFrame(results)\n",
    "print(resultsdf.sort_values(by='r2 score', ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
