{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba76d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('creditcard.csv')\n",
    "print(\"This is the unscaled data set. ALl are PCA scaled except time and amount.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new features i.e. feature engineering\n",
    "df['amount-time']=df['Amount']/(df['Time']+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b441fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know there are no missing values as from eda before\n",
    "# Scaling\n",
    "\n",
    "scaler=StandardScaler()\n",
    "df['Amount']=scaler.fit_transform(df['Amount'].values.reshape(-1,1)) \n",
    "# here reshape converts the column in to the numpy array as fit transform expects a 2d array\n",
    "\n",
    "df['amount-time']=scaler.fit_transform(df[['amount-time']])\n",
    "\n",
    "df['Time']=scaler.fit_transform(df[['Time']])\n",
    "# here there is no reshape as df[[]'time']] itself selects the column as the 2d array\n",
    "print(\"Now the Time and Amount columns are also scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "\n",
    "x=df.drop(['Class'], axis=1)\n",
    "y=df['Class']\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(x,y, test_size=0.2,stratify=y, random_state=42)\n",
    "'''  \n",
    "our dataset is imbalanced, we have 99% not fraud and 1% fraud \n",
    "so due to this on splitting on train and test it will lead to the disproportion so\n",
    "to make the equal proportion of fraud and non fraud in train and test sample we use stratify=y\n",
    "\n",
    "the order of output from the train_test_split is x_train, x_test, y_train, y_test as it gives features train and test and then the label train and test\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTe oversampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# smote is synthetic minority oversampling technique that creates new samples of minority class\n",
    "\n",
    "# initializing smote object with a fixed random state for reproducibility\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_over, y_train_over= smote.fit_resample(x_train, y_train) \n",
    "''' \n",
    " this does 2 things as fit learns the feature space of minority class i.e. fraud class\n",
    " and resample generates new samples of minority class  to balance the class and datasets\n",
    "'''\n",
    "print(\"before smot\",y_train.value_counts())\n",
    "print(\"after smot\",y_train_over.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for the smote  to build the model\n",
    "\n",
    "\n",
    "model= LogisticRegression(max_iter=1000, random_state=42)\n",
    "# this max_iter controls how many times the model iterates and finds the value\n",
    "model.fit(x_train_over,y_train_over)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Logistic Regression Model\")\n",
    "\n",
    "# now evaluating the model i.e. classification model so we need classification reports for logistic regression\n",
    "print(\"Classification Report:\")\n",
    "cr=classification_report(y_test, y_pred)\n",
    "print(cr)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Viusalizing the metrics using heatmap from seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
    "\n",
    "''' \n",
    "annot gives number inside the box along with the color\n",
    "fmt='d' formats the annotation as integers\n",
    "xtickables are used to show the labels in x axis and ytickables are used to show the labels in y axis so as we know which box is what\n",
    "\n",
    "'''\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now random forest for the smote \n",
    "\n",
    "# making object of model\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "rf_smote.fit(x_train_over, y_train_over)\n",
    "\n",
    "# predicting \n",
    "y_pred_rf_smote = rf_smote.predict(x_test)\n",
    "\n",
    "print(\"Random Forest Classifier Model with SMOTE\\n\")\n",
    "\n",
    "# now evaluating the model\n",
    "print(\"Confusion Matrix for Random Forest of SMOTE:\")\n",
    "cm_rf_smote = confusion_matrix(y_test, y_pred_rf_smote)\n",
    "print(cm_rf_smote)\n",
    "print(\"\\nClassification Report for Random Forest of SMOTE:\")\n",
    "cr_rf_smote = classification_report(y_test, y_pred_rf_smote)\n",
    "print(cr_rf_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Under Sampling \n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# this reduces the majorityy class in training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_train_under, y_train_under = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"before under sampling\", y_train.value_counts())\n",
    "print(\"after under sampling\", y_train_under.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59277775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regressor for the under sampling\n",
    "\n",
    "# object of model\n",
    "modell=LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# this learns from the undersampled training data \n",
    "modell.fit(x_train_under, y_train_under)\n",
    "\n",
    "# this predicts the test datas\n",
    "y_pred_under = modell.predict(x_test)\n",
    "\n",
    "print(\"\\nLogistic Regression Model with Under Sampling\")\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "cm =confusion_matrix(y_test, y_pred_under)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "cr=classification_report(y_test, y_pred_under)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using the random forest regressor for the undersampling\n",
    "\n",
    "# making the object for the model\n",
    "print('\\nNow Lets do Random Forest Classifier for undersampling.')\n",
    "rf= RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "rf.fit(x_train_under, y_train_under)\n",
    "y_pred_rf = rf.predict(x_test)\n",
    "\n",
    "# now evaluating the mode\n",
    "print(\"\\nConfusion Matrix for Random Forest of undersampling:\")\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm_rf)\n",
    "\n",
    "print(\"\\nClassification Report for Random Forest of undersampling:\")\n",
    "cr_rf = classification_report(y_test, y_pred_rf)\n",
    "print(cr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTETOMEK and  SMOTEENN\n",
    "\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "sample=10000\n",
    "x_Small= x_train.sample(n=sample, random_state=42)\n",
    "y_Small=y_train.loc[x_Small.index]\n",
    "\n",
    "\n",
    "smT = SMOTETomek(random_state=42)\n",
    "# This is only done in the training data\n",
    "x_st, y_St=smT.fit_resample(x_Small, y_Small)\n",
    "print(\"before smote tomek\", y_train.value_counts())\n",
    "print(\"after smote tomek\", y_St.value_counts())\n",
    "\n",
    "\n",
    "smenn= SMOTEENN(random_state=42)\n",
    "x_st_enn, y_st_enn= smenn.fit_resample(x_Small, y_Small)\n",
    "print(\"before smote enn\", y_train.value_counts())\n",
    "print(\"after smote enn\", y_st_enn.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic and random forest classifier for this smoteTomek and SMOTEENN    \n",
    "\n",
    "# random forest FOR SMOTE\n",
    "rf = RandomForestClassifier(random_state=42,class_weight='balanced')\n",
    "rf.fit(x_st, y_St)\n",
    "\n",
    "# THRESHOLD REDUCING\n",
    "y_prob_St= rf.predict_proba(x_test)[:, 1]\n",
    "threshold=0.3\n",
    "y_pred_rf = (y_prob_St >= threshold).astype(int)\n",
    "\n",
    "# this is for smootetmoek result\n",
    "print(\"Random Forest Classifier smote tomek by reducing threshold.\\n\")\n",
    "print('\\n')\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Confusion report for the random forest classifier\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix for Random Forest Classifier\")\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"rf\",cm_rf)\n",
    "\n",
    "\n",
    "# Random Forest for SMOTEENN\n",
    "\n",
    "print('\\n Random forest for the smoteen')\n",
    "\n",
    "# hyperparameter tuning using GridSearchCV\n",
    "param={\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None], # none means no limit of the depth of the tree\n",
    "    'min_samples_split': [2, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_rf_enn= GridSearchCV( estimator= rf,\n",
    "                          param_grid=param,\n",
    "                          scoring='recall', # this is used to maximize recall which is much needed in fraud detection\n",
    "                          cv=3,\n",
    "                          n_jobs=1, # this uses all available cpu cores to run in parallel\n",
    "                          verbose=1 # this means transparency of the updates of each parameters evaluation\n",
    ")\n",
    "\n",
    "\n",
    "# fiting this on the smoteenn data\n",
    "grid_rf_enn.fit(x_st_enn,y_st_enn)\n",
    "\n",
    "# after fitting using hyperparameter, now we get the best model from GridSearch\n",
    "best_rf_enn= grid_rf_enn.best_estimator_\n",
    "\n",
    "\n",
    "# this is using the threshold for the probability of the random forest\n",
    "y_prob_st_enn = best_rf_enn.predict_proba(x_test)[:, 1]\n",
    "y_pred_rf_enn = (y_prob_st_enn >= threshold).astype(int)\n",
    "\n",
    "# evaluating model\n",
    "print(\"GridSearchCv results: \\n\")\n",
    "print(\"best parameters:\", grid_rf_enn.best_params_)\n",
    "print(\"best cv recall score: \",grid_rf_enn.best_score_)\n",
    "\n",
    "\n",
    "print(\"classification report for Random Forest Classifier with SMOTEENN\")\n",
    "print(classification_report(y_test, y_pred_rf_enn))\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest Classifier with SMOTEENN\")\n",
    "cm_rf_enn = confusion_matrix(y_test, y_pred_rf_enn)\n",
    "print(\"rf enn\", cm_rf_enn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using some robust model for increasing recall \n",
    "\n",
    "# using xgboost for the smote and tomek \n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# creating object for the model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(x_st, y_St)\n",
    "\n",
    "\n",
    "# Threshold reducing for xgboost\n",
    "y_prob_xgb = xgb_model.predict_proba(x_test)[:, 1]\n",
    "threshold_xgb = 0.3\n",
    "y_pred_xgb = (y_prob_xgb >= threshold_xgb).astype(int)\n",
    "\n",
    "print(\"\\nXGBoost Classifier Model with SMOTE and Tomek Links\\n\")\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Confusion Matrix for XGBoost Classifier with SMOTE and Tomek Links:\\n\")\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(cm_xgb)\n",
    "print(\"\\nClassification Report for XGBoost Classifier with SMOTE and Tomek Links:\")\n",
    "cr_xgb = classification_report(y_test, y_pred_xgb)\n",
    "print(cr_xgb)\n",
    "\n",
    "\n",
    "# using xgboost for the smote enn\n",
    "\n",
    "# making object of model\n",
    "xgb_model_enn = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "xgb_model_enn.fit(x_st_enn, y_st_enn)\n",
    "\n",
    "# THRESHOLD REDUCING for xgboost with SMOTEENN\n",
    "y_prob_xgb_enn = xgb_model_enn.predict_proba(x_test)[:, 1]\n",
    "y_pred_xgb_enn = (y_prob_xgb_enn >= threshold_xgb).astype(int)\n",
    "\n",
    "\n",
    "print(\"\\nXGBoost Classifier Model with SMOTEENN\\n\")\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Confusion Matrix for XGBoost Classifier with SMOTEENN:\\n\")\n",
    "cm_xgb_enn = confusion_matrix(y_test, y_pred_xgb_enn)\n",
    "print(cm_xgb_enn)\n",
    "print(\"\\nClassification Report for XGBoost Classifier with SMOTEENN:\")\n",
    "cr_xgb_enn = classification_report(y_test, y_pred_xgb_enn)\n",
    "print(cr_xgb_enn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
